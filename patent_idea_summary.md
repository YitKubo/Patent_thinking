# MTAD-GATにRAM名の意味類似度を統合した異常検知手法

## 1. 技術概要

### 発明のポイント
- **MTAD-GAT**（多変量時系列異常検知）に**RAM名の意味的類似度**を統合
- **BART**による自然言語埋め込み + **コサイン類似度** + **Graph Attention機構**の融合
- チャンネル名の意味を活用した欠損耐性・ロバスト性の向上

## 2. 先行技術との関係

### ①CNNベース異常検知（先行特許）
- **課題**: 局所時系列パターンは捉えられるが、チャンネル間の意味的関係を反映できない
- **限界**: センサー名・RAM名に込められた意味を活用不可

### ②MTAD-GATベース（自己先行出願）
- **利点**: チャンネル間の時系列相関をグラフ構造で表現
- **課題**: ラベル名の意味的関係を考慮できない
- **例**: `RAM_voltage`と`RAM_current`が意味的に近くても、統計的相関が低ければ結合されない

### ③今回の提案手法
- **解決**: ②を基盤として、①②両方の課題を克服
- **効果**: 統計相関 + 意味類似度の二重グラフ表現

## 3. 技術の流れ

### 全体フロー
```mermaid
flowchart TD
    A[RAMチャンネル名<br/>例: RAM_voltage, RAM_speed] --> B[BARTエンコーダ]
    B --> C[埋め込みベクトル e_i]
    C --> D[コサイン類似度計算<br/>S_ij = cos(e_i, e_j)]
    D --> E[類似度行列 S]
    
    subgraph GAT拡張
        X[入力系列データ X_t] --> Y[GAT注意機構]
        E --> Y
        Y --> Z[拡張Attention重み α_ij]
        Z --> O[異常スコア判定]
    end
```

### 数式表現

#### 1. RAM名の意味ベクトル化
```
e_i = BARTEncoder(RAM_i)
```
- トークン数が可変でも**平均プーリング**で固定長ベクトルに統一

#### 2. 類似度行列
```
S_ij = cos(e_i, e_j) = (e_i^T × e_j) / (||e_i|| × ||e_j||)
```

#### 3. グラフ注意の拡張
通常のGATロジット：
```
ℓ_ij = (W_Q × h_i)^T × (W_K × h_j) / √d
```

**提案手法**（類似度バイアス追加）：
```
ℓ'_ij = ℓ_ij + λ×A_ij^(time) + τ×S_ij
```

Softmax で注意係数算出：
```
α_ij = softmax_j(ℓ'_ij)
```

## 4. 実装アプローチ

### 方法A：注意ロジットへの加点バイアス（主実施例）
```python
# 疑似コード
logits = scaled_dot_product_attention(Q, K)  # 通常のGAT
logits = logits + tau * S_bias  # 意味類似度バイアス追加
attn = softmax(logits)
```

### 方法B：ノード特徴への直接埋め込み（変形例）
```python
# チャンネル特徴に意味ベクトルを結合
x_enhanced = concat([x_time_series, e_semantic], dim=-1)
# 通常のGATで処理
output = GAT(x_enhanced, A_time)
```

## 5. トークン分割問題への対処

### 課題
- `RAM_voltage` → `["RAM", "_vol", "tage"]` に分割される
- チャンネルごとにトークン数が異なる

### 解決策
1. **平均プーリング**: 全トークンベクトルの算術平均
2. **[CLS]トークン利用**: 先頭トークンの表現
3. **最大値プーリング**: 各次元の最大値
4. **Attentionプーリング**: 学習可能な重み付け

### 実施例（特許明細書風）
> 生成された複数のトークン埋め込みベクトルは、プーリング処理によって単一の固定長ベクトルに集約される。本実施例では平均化処理を用い、全トークンベクトルの算術平均を算出することにより、チャンネルごとに一貫した次元の埋め込みベクトルを得る。

## 6. 複数チャンネルでの類似度行列

### 基本設計
- **N個のチャンネル** → **N×N の類似度行列S**
- 全チャンネル間のペアワイズ計算
- Top-k疎化で計算効率とノイズ除去

### 疎化・正規化
```python
# 正規化 + top-k疎化
S = cosine_similarity(embeddings)  # [N, N]
S = np.maximum(S, 0.0)  # 負値除去
S_norm = minmax_scale(S)  # 0-1正規化

# top-k疎化（各行で上位k個のみ保持）
S_sparse = apply_topk_mask(S_norm, k=5)
```

## 7. 特許性の評価

### 新規性
- ✅ 「異常検知」×「時系列モデル」×「RAM名ベクトル類似度」の組合せ
- ✅ BARTなどの言語モデルをAttentionバイアスに活用する構成

### 進歩性  
- ✅ 単純なone-hotではなく、自然言語由来の意味ベクトル利用
- ✅ 欠損チャンネル補完・関連性強化という明確な技術的効果

### 産業上の利用可能性
- ✅ 自動車ECU、産業プラント、ITシステム監視への応用

## 8. 先行技術文献の扱い

### 自己先行出願の引用
```
【先行技術文献】
特許文献1：特開202X-XXXXX号公報（CNNベース異常検知技術）
特許文献2：特開202X-YYYYY号公報（出願人：結人 久保、MTAD-GATを用いた異常検知技術）
```

### 優先権戦略
- **国内優先権**: 最初の出願から1年以内
- **PCT出願**: パリ条約優先権を活用
- **分割出願**: 審査過程で複数発明に分割可能

## 9. 請求項の構成案

### 主請求項（広範囲）
「多変量時系列データの各チャンネル名を自然言語モデルにより埋め込み、得られた類似度をGraph Attention Networkの注意重みに組み込む異常検知方法」

### 従属請求項（具体化）
- 自然言語モデルがBARTである方法
- 類似度がコサイン類似度である方法  
- top-k疎化を適用する方法
- 平均プーリングを用いる方法

## 10. 発明の効果

### 技術的効果
1. **意味的関係の活用**: 名称が類似するチャンネル間の結合強化
2. **欠損耐性の向上**: 関連チャンネルからの情報補完
3. **拡張性**: 新チャンネル追加時の自動統合
4. **汎化性能**: 統計相関＋意味類似度の二重表現

### 応用領域
- 自動車: ECU・RAMログ監視
- 産業: プラント設備監視  
- IT: システムメトリクス異常検知
- IoT: センサーネットワーク監視

## 11. 出願戦略

### 包括出願 → 分割戦略
1. **初回出願**: 方法A・B両方を含む包括的な出願
2. **分割検討**: 審査官指摘や戦略的判断で分割
3. **国際展開**: PCT → 主要国移行

### 代替モデルへの対応
- **請求項**: 「自然言語モデル」として広く記載
- **実施例**: BART, BERT, Sentence-BERT, SimCSE等を列挙
- **将来対応**: GPT系埋め込みモデルにも適用可能

---

## まとめ

本発明は、従来のCNNベース（①）とMTAD-GATベース（②）の限界を克服し、**チャンネル名の意味情報をGraph Attentionに統合**する新規な異常検知手法である。特許性は十分に確保され、産業応用範囲も広い。自己先行出願を活用した優先権戦略により、国内・国際両面での権利化が期待できる。